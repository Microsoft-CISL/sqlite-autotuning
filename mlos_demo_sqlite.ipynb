{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19b0472b",
   "metadata": {},
   "source": [
    "# Azure Data University: mlos_bench SQLite data analysis (Student's workbook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6d54c6-5688-4b7b-a8fc-5c83bfd1d308",
   "metadata": {},
   "source": [
    "In this notebook, we look at the data from the `mlos_bench` experiment we run in class to find a better SQLite configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c05a23-2ce8-4826-bc00-93431e4fd7e6",
   "metadata": {},
   "source": [
    "### 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffd2cbd",
   "metadata": {},
   "source": [
    "Before using this notebook, run the following commands in the integrated terminal of this codespace:\n",
    "\n",
    "```sh\n",
    "conda activate mlos\n",
    "\n",
    "mlos_bench --config \"./config/cli/local-sqlite-opt.jsonc\" --globals \"./config/experiments/sqlite-sync-journal-pagesize-caching-experiment.jsonc\" --max-iterations 100\n",
    "```\n",
    "\n",
    "> See Also: [README.md](./README.md) for further instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd879e7",
   "metadata": {},
   "source": [
    "After letting it run for a few trials (it should take 10 to 15 minutes), we can start analyzing the autotuning data produced by the `mlos_bench` framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695ebcd9-9c15-4516-9306-bb4bbe2fcb6f",
   "metadata": {},
   "source": [
    "### 2. Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed6f16-748b-4aa2-bd97-15b06395b7a2",
   "metadata": {},
   "source": [
    "First, we import a few common third-party libraries for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aadee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a few popular third-party packages for data analysis. They come pre-installed with Anaconda.\n",
    "import pandas\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc20772-fdeb-415d-82af-dae67a307965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import mlos_bench Storage API to access the experimental data.\n",
    "from mlos_bench.storage import from_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976ad1ab-17d3-4d10-b07b-200e3b8d0fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosmetic: Suppress some annoying warnings from third-party data visualization packages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddb84fb",
   "metadata": {},
   "source": [
    "### 3. Connect to the DB using existing mlos_bench configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fab05f-7aff-4a97-ba33-3e159a566938",
   "metadata": {},
   "source": [
    "We reuse the existing mlos_bench framework configuration file that contains the DB connection parameters. This way we make sure to connect to the same database that our framework uses to store the experimental data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccca92e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = from_config(config_file=\"storage/sqlite.jsonc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa2feab",
   "metadata": {},
   "source": [
    "### 4. Load the data for our experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4956fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage.experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428c5ac1-6be2-4f02-a425-932a5f12973a",
   "metadata": {},
   "source": [
    "You should see a record for our experiment in the DB. Let's look at the data associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb4baf2-adca-4f86-acb9-5cfeb2b63e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = \"sqlite-opt-demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fae2e0b",
   "metadata": {},
   "source": [
    "### 5. Get all data for one experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c525fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = storage.experiments[experiment_id]\n",
    "exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29cc04a-2736-4ee4-a461-61a3784e8e8e",
   "metadata": {},
   "source": [
    "Main method that combines the information about each trial along with the trial configuration parameters and its results, is the property `.results`. It conveniently returns all data about the experiment is a single Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d3f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View some of the result data associated with that experiment.\n",
    "df = exp.results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e3545d-67a1-4fa1-b502-e3ea5a5e7666",
   "metadata": {},
   "source": [
    "Each record of the DataFrame has the information about the trial, e.g., its timestamp and status, along with the configuration parameters (columns prefixed with `config.`) and the benchmark results (columns prefixed with `result.`). Let's look at the first record to see all these fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599843df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613f5d14",
   "metadata": {},
   "source": [
    "### 6. Plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100dd004",
   "metadata": {},
   "source": [
    "#### 6.1. Plot the behavior of the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ef3911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1. plot of the optimizer's best config found so far (convergence rate of the optimizer)\n",
    "# 2. plot of the performance of each config the optimizer found (search behavior of the optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32680002",
   "metadata": {},
   "source": [
    "#### 6.2. Plot the results of one metric vs. another for some tunable\n",
    "\n",
    "The intent is to explore parameter importance and impact on different metrics (both application performance and system resource usage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d850fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical tunable to consider:\n",
    "CATEGORY = \"config.synchronous\"\n",
    "\n",
    "# A system resource metric to analyze.\n",
    "METRIC = \"result.File system outputs\"\n",
    "\n",
    "# Which performance metric to plot on the Y-axis.\n",
    "SCORE = \"result.90th Percentile Latency (microseconds)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538a56b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (6, 5)\n",
    "\n",
    "sns.scatterplot(data=df, x=METRIC, y=SCORE, hue=CATEGORY, marker='o', alpha=0.6)\n",
    "\n",
    "plt.title(\"Experiment: \" + exp.exp_id)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3f124d-ff82-41dc-84d3-44ee68f238dc",
   "metadata": {},
   "source": [
    "The results are here, but the outliers make it really difficult to understand what's going on. Let's switch to the log scale and see if that helps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f1d31d-623f-4964-9fd3-67c767a17d3d",
   "metadata": {},
   "source": [
    "#### 6.3. Plot on log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61928df-d874-4ff4-8c41-fb1032f1d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (6, 5)\n",
    "\n",
    "sns.scatterplot(data=df, x=METRIC, y=SCORE, hue=CATEGORY, marker='o', alpha=0.6)\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.title(\"Experiment: \" + exp.exp_id)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb9ffdd-5fe1-4a57-b5db-cb48f91e8f16",
   "metadata": {},
   "source": [
    " Now we can see that setting `synchronous=off` seems to seems to improve the latency a lot. Apparently, the optimizer had also noticed that and focused on exploring a particular area of the configuration space. Let's switch back to the linear scale and zoom in to that region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29de8933-1765-4bc7-a46e-97ffc5915e4b",
   "metadata": {},
   "source": [
    "#### 6.4. Zoom in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd27dc5-85ee-4f92-8a34-483535cce85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
    "\n",
    "sns.scatterplot(data=df, x=METRIC, y=SCORE, hue=CATEGORY, marker='o', alpha=0.7)\n",
    "\n",
    "plt.xlim(100000, 350000)\n",
    "plt.ylim(950, 1250)\n",
    "\n",
    "plt.title(\"Experiment: \" + exp.exp_id)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6120b4d2-88f3-4aca-bb20-808688b1615d",
   "metadata": {},
   "source": [
    "The latency seems to be minimal when the configuration parameter `synchronous` is set to `off` and when the metric `File system outputs` is in range of 100..300K. Let's focus on that subset of data and see what other configuration settings get us there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68939d4c-bcab-494e-bbc6-28a4faffa613",
   "metadata": {},
   "source": [
    "#### 6.5. Look at other configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e7f032-8a4e-49d6-928b-250485d4d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lim = df[(df[CATEGORY] == \"off\") & (df[METRIC] > 100000) & (df[METRIC] < 300000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1f6358-ba12-41bf-871f-fa80bb944dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
    "\n",
    "sns.scatterplot(data=df_lim, x=\"config.cache_size\", y=SCORE, hue=\"config.journal_mode\", marker='o', alpha=0.7)\n",
    "\n",
    "plt.title(\"Experiment: \" + exp.exp_id)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f053367-5285-4352-9285-3ded1408b976",
   "metadata": {},
   "source": [
    "Again, we see that setting `journal_mode` to `wal` and `cache_size` to the value between 500MB and 2GB seem to produce good results, but we need more experiments to explore that hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7600f6a6-ed11-408b-baca-ae13de0803b8",
   "metadata": {},
   "source": [
    "### 7. Outro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20656b8-092f-4e89-abc3-eccdba63326d",
   "metadata": {},
   "source": [
    "If you feel curious, please go ahead and play with the SQLite data in the cells below.\n",
    "\n",
    "After that, please open other notebooks in this repository and explore the data you have collected in this class as well as the results from our MySQL optimization experiments:\n",
    "\n",
    "* [**mlos_demo_sqlite_teachers.ipynb**](mlos_demo_sqlite_teachers.ipynb) - Teacher's copy, don't peek! :-) Here we analyze the data from 100 trials of SQLite optimization we ran in this codespace before the class. The results you get in the workshop should look similar.\n",
    "* [**mlos_demo_mysql.ipynb**](mlos_demo_mysql.ipynb) - Look at the actual production data we've collected in serveral experiment for MySQL Server optimization on Azure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb01b48-e8e3-4f20-9027-b0f6371e5d15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
