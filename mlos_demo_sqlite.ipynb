{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19b0472b",
   "metadata": {},
   "source": [
    "# Azure Data University: mlos_bench SQLite data analysis (Student's workbook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6d54c6-5688-4b7b-a8fc-5c83bfd1d308",
   "metadata": {},
   "source": [
    "In this notebook, we look at the data from 100 trials we ran in `mlos_bench` to find a better SQLite configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c05a23-2ce8-4826-bc00-93431e4fd7e6",
   "metadata": {},
   "source": [
    "### 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffd2cbd",
   "metadata": {},
   "source": [
    "We used the following commands in the integrated terminal of this codespace:\n",
    "\n",
    "```sh\n",
    "conda activate mlos\n",
    "\n",
    "mlos_bench --config config/cli/local-sqlite-opt.jsonc \\\n",
    "           --globals config/experiments/sqlite-sync-journal-pagesize-caching-experiment.jsonc \\\n",
    "           --max-iterations 100\n",
    "```\n",
    "\n",
    "> See Also: [README.md](./README.md) for further instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd879e7",
   "metadata": {},
   "source": [
    "After letting it run for a few trials (it should take 10 to 15 minutes), we can start analyzing the autotuning data produced by the `mlos_bench` framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695ebcd9-9c15-4516-9306-bb4bbe2fcb6f",
   "metadata": {},
   "source": [
    "### 2. Import MLOS packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc20772-fdeb-415d-82af-dae67a307965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import mlos_bench Storage API to access the experimental data.\n",
    "from mlos_bench.storage import from_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddb84fb",
   "metadata": {},
   "source": [
    "### 3. Connect to the DB using existing mlos_bench configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fab05f-7aff-4a97-ba33-3e159a566938",
   "metadata": {},
   "source": [
    "We reuse the existing `mlos_bench` framework configuration file that contains the DB connection parameters.\n",
    "This way we make sure to connect to the same database that our framework uses to store the experimental data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccca92e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = from_config(config_file=\"storage/sqlite.jsonc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa2feab",
   "metadata": {},
   "source": [
    "### 4. Load the data for our experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4850663",
   "metadata": {},
   "source": [
    "At the top level, Storage API has a single property, `.experiments` that returns a Python `dict` of key/value pairs of Experiment ID and Experiment Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4956fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage.experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428c5ac1-6be2-4f02-a425-932a5f12973a",
   "metadata": {},
   "source": [
    "You should see a record for our experiment in the DB. Let's look at the data associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb4baf2-adca-4f86-acb9-5cfeb2b63e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment_id = \"sqlite-sync-journal-pagesize-caching-experiment\"\n",
    "experiment_id = \"sqlite-opt-demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fae2e0b",
   "metadata": {},
   "source": [
    "### 5. Get all data for one experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c525fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = storage.experiments[experiment_id]\n",
    "display(exp)\n",
    "exp.objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9571ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the set of optimization target objectives.\n",
    "display(exp.objectives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29cc04a-2736-4ee4-a461-61a3784e8e8e",
   "metadata": {},
   "source": [
    "Main method that combines the information about each trial along with the trial configuration parameters and its results, is the property `.results`. It conveniently returns all data about the experiment is a single Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d3f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = exp.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42395b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print the first 10 records of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e3545d-67a1-4fa1-b502-e3ea5a5e7666",
   "metadata": {},
   "source": [
    "Each record of the DataFrame has the information about the trial, e.g., its timestamp and status, along with the configuration parameters (columns prefixed with `config.`) and the benchmark results (columns prefixed with `result.`). The `trial_id` field is simply the iteration number within the current experiment. Let's look at the first record to see all these fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599843df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print a single record of the `df` DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a07ea9c",
   "metadata": {},
   "source": [
    "#### 5.1. Look at the data\n",
    "\n",
    "We can think of each trial as a _noisy_ black-box function that has multiple inputs (that is, `config.*` parameters) and multiple outputs (the `result.*` fields). One of those outputs is designated as a target optimization metric. In our case, it's the DataFrame column named `result.90th Percentile Latency (microseconds)`, but we can reuse other outputs in different experiments (e.g., finding a configuration for maximizing throughput instead of minimizing latency).\n",
    "\n",
    "The goal of our optimization process is to find input values (that is, the configuration) that minimize the output score, i.e., the 90th percentile query latency. The optimizer repeatedly proposes the new input values to efficiently explore the multi-dimensional configuration space and find the (global) optimum.\n",
    "\n",
    "Of course, we can just blindly trust the optimizer and just use configuration it recommends as an optimum after some reasonably large series of trials; however, it is always a good idea to look at the data from all trials and try to better understand the behavior of the system and see how each configuration parameter impacts its performance. Such multi-dimensional data analysis is a daunting task, but looking at one or two dimensions at a time can already reveal a lot of information.\n",
    "\n",
    "We'll do that in the sections below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b3b26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use Pandas API to print a few more records or columns of the data.\n",
    "# Can you see the correlation between the configuration parameters and the results?\n",
    "# Neither can we."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd87ddb3",
   "metadata": {},
   "source": [
    "### 6. Visualize the results data automatically using `mlos_viz.plot()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bad8fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlos_viz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a0fc88",
   "metadata": {},
   "source": [
    "`mlos_viz` attempts to use the information about the data to automatically provide some basic visualizations without much effort on the developer's part.\n",
    "\n",
    "At the moment, we do this using [`dabl`](https://github.com/dabl/dabl), though in the future we intend to add support for more interactive visualizations or even scheduling new trials, while maintaining a very simple API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb651296",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlos_viz.plot(exp, filter_warnings=True)    # cosmetic - attempt to hide some noisy warnings from underlying libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6630fa1a",
   "metadata": {},
   "source": [
    "### TODO: Write up take aways from these graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613f5d14",
   "metadata": {},
   "source": [
    "### 7. Alternatively, plot the results manually using `seaborn` and `matplotlib`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735193f4",
   "metadata": {},
   "source": [
    "We will use a few common third-party libraries for data analysis. If you have never used them before, don't worry! This workshop would be a great opportunity for you to learn the basics. Here are some useful resources that you might want to open is a separate tab to keep as a reference during this class.\n",
    "* [Pandas](https://pandas.pydata.org) - A package to work with tabular data in Python.\n",
    "* [Matplotlib](https://matplotlib.org) - Most popular Python plotting library, powerful, but somewhat low-level by modern standards.\n",
    "* [Seaborn](https://seaborn.pydata.org) - High-level data visualization package on top of Matplotlib.\n",
    "\n",
    "All these packages come pre-installed with Python Anaconda distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aadee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a few popular third-party packages for data analysis. They come pre-installed with Anaconda.\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976ad1ab-17d3-4d10-b07b-200e3b8d0fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosmetic: Suppress some annoying warnings from third-party data visualization packages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d7b37e",
   "metadata": {},
   "source": [
    "Let's look at some configuration parameters and benchmark metrics we have in the DataFrame. Here are the DataFrame columns that we suggest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d850fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical tunable to consider:\n",
    "CATEGORY = \"config.synchronous\"\n",
    "\n",
    "# A system resource metric to analyze.\n",
    "METRIC = \"result.File system outputs\"\n",
    "\n",
    "# Which performance metric to plot on the Y-axis.\n",
    "SCORE = \"result.90th Percentile Latency (microseconds)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100dd004",
   "metadata": {},
   "source": [
    "#### 7.1. Plot the behavior of the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70b61d3",
   "metadata": {},
   "source": [
    "First, let's see the benchmark results on each iteration.\n",
    "\n",
    "* Build a scatterplot with the iteration number `trial_id` on X axis and the `SCORE` column on Y.\n",
    "> You can easily do that with the [`sns.scatterplot()`](https://seaborn.pydata.org/generated/seaborn.scatterplot.html) function from [Seaborn](https://seaborn.pydata.org)\n",
    "* On the same plot, add a line showing the best (i.e., minimal) score we have so far on each iteration.\n",
    "> You can use [Pandas](https://pandas.pydata.org) method [`.cummin()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.cummin.html) to get a cumulative minimum of the column.\n",
    "> Use [Seaborn](https://seaborn.pydata.org) function [`sns.lineplot()`](https://seaborn.pydata.org/generated/seaborn.lineplot.html) to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cce6cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10, 4)  # Set the picture size to some reasonable values\n",
    "\n",
    "# TODO: 1. Build a scatterplot with the iteration number trial_id on X axis and the SCORE column on Y.\n",
    "# `sns.scatterplot()` is one function to do it.\n",
    "\n",
    "# TODO: 2. Add a line showing the best (i.e., minimal) score we have so far on each iteration.\n",
    "# You may want to use Pandas `.cummin()` method and `sns.lineplot()` from Seaborn.\n",
    "\n",
    "# HINT: use plt.yscale('log') if the Y axis data it too spread out.\n",
    "\n",
    "# HINT: Use plt.xlabel and plt.ylabel to give some nice human-readable names to each axis of the plot.\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef318739-ba2c-4898-81d4-1e1422f5ee57",
   "metadata": {},
   "source": [
    "We should see that the optimizer finds a good configuration in about 10 interations.\n",
    "After that, it oscillates between exploring some remote areas of the configuration space (and getting some pretty bad banchmarking results) and exploiting the neighborhoods of the well-performing configurations, further improving the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32680002",
   "metadata": {},
   "source": [
    "#### 7.2. Plot the results of one metric vs. another for some tunable\n",
    "\n",
    "The intent is to explore parameter importance and impact on different metrics (both application performance and system resource usage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b6259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (6, 5)  # Set the picture size to some reasonable values\n",
    "\n",
    "# TODO: 1. Build another scatterplot to plot the `METRIC` against the `SCORE`.\n",
    "\n",
    "# TODO: 2. add `hue=CATEGORY` parameter to visualize the additional configuration parameter.\n",
    "# (Which is \"config.synchronous\", as we've defined in p. 6).\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3f124d-ff82-41dc-84d3-44ee68f238dc",
   "metadata": {},
   "source": [
    "The results are here, but the outliers make it really difficult to understand what's going on. Let's switch to the log scale and see if that helps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f1d31d-623f-4964-9fd3-67c767a17d3d",
   "metadata": {},
   "source": [
    "#### 7.3. Plot on log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c5e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (6, 5)  # Set the picture size to some reasonable values\n",
    "\n",
    "# TODO: Use the same code as above, but add `plt.xscale(\"log\")` and/or `plt.yscale(\"log\")`, like we did in 6.1.\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb9ffdd-5fe1-4a57-b5db-cb48f91e8f16",
   "metadata": {},
   "source": [
    "Now you should see that setting `synchronous=off` seems to improve the latency a lot, which makes sense - less waiting on the disk. Does it increase any other resource usage?\n",
    "Apparently, the optimizer had also noticed that and focused on exploring a particular area of the configuration space. Let's switch back to the linear scale and zoom in to that region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29de8933-1765-4bc7-a46e-97ffc5915e4b",
   "metadata": {},
   "source": [
    "#### 7.4. Zoom in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafc1d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
    "\n",
    "# TODO: Use the same code as above, but switch back to the linear scale, and use\n",
    "# Matplotlib `plt.xlim()` and `plt.ylim()` functions to limit the area being visualized.\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d3a989",
   "metadata": {},
   "source": [
    "> **NOTE:** If you have your experiment still running in the background, now is a good time to re-run all the cells above and reload the data, so you can have a few more data points in your `df` DataFrame.\n",
    "\n",
    "You should see that the latency seems to be minimal when the configuration parameter `synchronous` is set to `off` and when the metric `File system outputs` is in range of 100..300K. Let's focus on that subset of data and see what other configuration settings get us there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68939d4c-bcab-494e-bbc6-28a4faffa613",
   "metadata": {},
   "source": [
    "#### 7.5. Look at other configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1217faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make sure that the range for `METRIC` and value of `CATEGORY` are correct.\n",
    "# Feel free to add or drop the filtering criteria in the statement below to focus on the right subset of the data.\n",
    "\n",
    "df_lim = df[(df[CATEGORY] == \"off\") & (df[METRIC] > 100000) & (df[METRIC] < 300000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7120dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
    "\n",
    "# TODO: Copy the code from our previous plot and build a scatterplot of `df_lim` using different column names\n",
    "# for `x` and `hue` parameters. # E.g., see how `config.cache_size` and `config.journal_mode` configuration\n",
    "# parameters impact the query latency.\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f053367-5285-4352-9285-3ded1408b976",
   "metadata": {},
   "source": [
    "Again, we should see that setting `journal_mode` to `wal` and `cache_size` to the value between 500MB and 2GB seem to produce good results, but we need more experiments to explore that hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7600f6a6-ed11-408b-baca-ae13de0803b8",
   "metadata": {},
   "source": [
    "### 8. Outro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20656b8-092f-4e89-abc3-eccdba63326d",
   "metadata": {},
   "source": [
    "If you feel curious, please go ahead and play with the SQLite data in the cells below.\n",
    "\n",
    "After that, please open other notebooks in this repository and explore the data you have collected in this class as well as the results from our MySQL optimization experiments:\n",
    "\n",
    "* [**mlos_demo_sqlite.ipynb**](mlos_demo_sqlite.ipynb) - Use this notebook to analyze the data you've collected during this workshop.\n",
    "* [**mlos_demo_mysql.ipynb**](mlos_demo_mysql.ipynb) - Look at the actual production data we've collected in serveral experiment for MySQL Server optimization on Azure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb01b48-e8e3-4f20-9027-b0f6371e5d15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
